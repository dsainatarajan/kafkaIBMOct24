Day7:
Kafka Streams
Kafka Advanced Admin Commands
Microservices Architecture for Kafka
Confluent Kafka
Confluent Control Center
Confluent Ksqldb

Kafka Streams		-> A stream processing application
	Is built on the Java Streams API
	Is a feature available in Apache Kafka itselt
	Java Producer, Consumer API Vs Stream processing API

Normal:
for(eleme e: list){
	if(e.value %2 ==0)
		sum += e.vale+10;
}

Stream processing using Streams API:
Stream<List<Int>> lStream = list.stream();
lStream.filter(e.value %2 == 0).sum();


High Level processing APIs offered by streams: 
Records are automatically streamed, we just handle one record from a stream:
	map, flatmap, filter, etc 
	Stateless
	
Aggregations with multiple records:
	groupBy, groupByKey, reduce, reduceByKey
	Statefull, window based operations
	
list.stream().map().filter().reduce()			-> Java Stream
StreamBuilder(kafkaTopic).map().filter().reduce()-> Kafka Streams
Kafka Streams API is part of the core apache Kafka

ETL Processing, Data Processing		-> Streams API is very helpful

StreamApp => 
	Input Topic	-> Processing		-> Result => Output Topic

Consumer	-> DeSerializer
Producer	-> Serializer
StreamsApp	-> SerDe		=> Serializer+DeSerializer




SimplePipe		-> Read From Source Topic -> Write to Target Topic
No Processing involved		=> Getting started exercise for Streams

Transform Exercise:
	Luxury Tax
	Any transaction over 200 -> 10% tax

key -> order1
value -> 250

map((k,v), pair(k, (v + v*0.1)))
v=500 => v = 500 + 500*0.1  => v=550

Kafka Stream Repo:
https://github.com/dsainatarajan/kafkastreamsjava

Kafka Streams -> Part of the apache(core) kafka
Producer & Consumer API: We handle one record at a time
Record by record processing will make it difficult for many processing requirements like
	i) Group by 
	ii) Group by with Aggregration(sum, avg, min, max, etc)
	iii) Join operation
	
Streams API is very helpful for ETL type of data processing workloads

stateless	-> Per Record processing =>map, filter, flatMap, etc
Statefull	-> Windowing operation, grouping & aggregations on window 
Adding/updating information from the past with new batch(information) is stateful operation -> to recover from a failure you need checkpoint(save) current state from some storage

StringSerilizer
StringDeSerilizer
StringSerde			-> serialization & deserialization
Kafka Streams works on top of the Java Streams

Simple Per Record processing	-> map
map function will get one record at a time as input and produces one output for each input

List list1 = 1,2,3,4,5
list1.map( oneVale -> oneVale*oneVale)

filter  -> filter will retain(send to output) the values we want, remove values that are supporsed to be filtered out
return value of the lambda logic should return a boolean
if the bool value is true, current value is retained in the output
if the bool value is false, current value is removed from the output


List list1 = 1,2,3,4,5
list1.filter( oneVale -> oneVale%2 == 0 )		-> retain even values
output =>  2, 4
1	-> 1%2 == 0 => output = false		-> removed 
2	-> 2%2 == 0 => output = true		-> retained 
3	-> 3%2 == 0 => output = false		-> removed 

List list1 = 1,2,3,4,5
list1.filter( oneVale -> oneVale%2 == 1 )		-> retain odd values
output =>  1,3,5

inputstream2 -> input topic

https://github.com/dsainatarajan/kafkastreamsjava


String.valueOf(Double.parseDouble(v.toString().trim())
    		   +Double.parseDouble(v.toString().trim())*0.1))
# 200	-> 200 + 200*0.10 => 200+20 -> 220


Offset commit:
Record1: <Key: "grp1, topic1, P0", Value: "offset:1">
Record2: <Key: "grp1, topic1, P0", Value: "offset:3">

Compaction  -> Delete all records with the same key and only retain the latest record for each key
After compation:
Record2: <Key: "grp1, topic1, P0", Value: "offset:3">


Saga Design Pattern: Event Driven Architecture
Distributed Transaction
	Committed transactions in each microservice cannot be undone, cannot be rolled back as it is already comitted
	Retore validity of data when there is a failed transaction?
		Compensating Transaction
		Eventually Consistent, intermediate incosistency will be there when there are failurs. Eventual consistency is restored through compensating transactions








